{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Logistic_Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlvqF7QBpDuTHX70trjOlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidBrynnHouse/NLP-Logistic-Regression/blob/main/NLP_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Yb4M-oebvFxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64dfefe-c976-4d15-c07a-9c29eb30f686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords        \n",
        "from nltk.stem import PorterStemmer        \n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import re\n",
        "from nltk.corpus import twitter_samples\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('twitter_samples')\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "m = len(pos_tweets) + len(neg_tweets)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"\n",
        "    attributes:\n",
        "    tweet: list of strings that represents a single tweet in the dataset\n",
        "    returns:\n",
        "    tweet: lowercased and removed stopwords, punctuations, handles, stemmed, and url's\n",
        "    \"\"\"\n",
        "\n",
        "    # instantiate stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    for word in tweet.split(' '):\n",
        "        # remove stopwords and stem\n",
        "        if word in stops:\n",
        "            tweet = tweet.replace(word, '')\n",
        "        else:\n",
        "            tweet = tweet.replace(word, stemmer.stem(word))\n",
        "\n",
        "\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "\n",
        "    # remove hashtags\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "    # lowercase tweet\n",
        "    tweet = str.lower(tweet)\n",
        "\n",
        "    # instantiate tokenizer class\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "    # tokenize tweet and remove punctuation\n",
        "    tweet = tokenizer.tokenize(tweet)\n",
        "\n",
        "    return tweet"
      ],
      "metadata": {
        "id": "_KR3VJyPtEPP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = 'Hello there Young man @How are you doing this morning? so archiving'\n",
        "print(preprocess_tweet(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMb5JSLmRgtr",
        "outputId": "c8a59e52-c671-44c0-9449-707c49422b7a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'ng', 'man', 'how', 'morning', 'archiv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocab_dict(pos_tweets, neg_tweets):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    vocab = {}\n",
        "    for tweet in pos_tweets:\n",
        "        for word in preprocess_tweet(tweet):\n",
        "            if vocab.get(word):\n",
        "                vocab[word] = (vocab[word][0] + 1, vocab[word][1])\n",
        "            else:\n",
        "                vocab[word] = (1, 0)\n",
        "    for tweet in neg_tweets:\n",
        "        for word in preprocess_tweet(tweet):\n",
        "            if vocab.get(word):\n",
        "                vocab[word] = (vocab[word][0], vocab[word][1] + 1)\n",
        "            else:\n",
        "                vocab[word] = (0, 1)\n",
        "    return vocab\n",
        "vocab = create_vocab_dict(pos_tweets, neg_tweets)"
      ],
      "metadata": {
        "id": "RTJv5COkRlQY"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_tweet(tweet):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    x = np.zeros((1, 3))\n",
        "    x[0,0] = int(1)\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    for word in tweet:\n",
        "        if vocab.get(word):\n",
        "            pos += int(vocab[word][0])\n",
        "            neg += int(vocab[word][1])\n",
        "    x[0,1] = int(pos)\n",
        "    x[0,2] = int(neg)\n",
        "    return x\n",
        "tweet = preprocess_tweet(pos_tweets[100])\n",
        "print(embed_tweet(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t9Xny2TTnPX",
        "outputId": "9de5b472-2a08-4fda-9587-e252ab5aef3e"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+00 2.271e+03 2.706e+03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.zeros((m, 3))\n",
        "for i in range(len(pos_tweets)):\n",
        "    tweet = preprocess_tweet(pos_tweets[i])\n",
        "    X[i, :]= embed_tweet(tweet)\n",
        "for j in range(len(neg_tweets)):\n",
        "    tweet = preprocess_tweet(neg_tweets[j])\n",
        "    X[len(pos_tweets) + j, :] = embed_tweet(tweet)   \n",
        "y_pos = np.ones((int(m/2), 1))\n",
        "y_neg = np.zeros((int(m/2), 1))\n",
        "y = np.append(y_pos, y_neg, axis=0)"
      ],
      "metadata": {
        "id": "RvMLSn47qcis"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "metadata": {
        "id": "dBCNoFIk367w"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    return 1/(1 + np.exp(-z))"
      ],
      "metadata": {
        "id": "TpELDYTB6Mtv"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradientDescent(X, y, theta, alpha, num_iter):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    for i in range(num_iter):\n",
        "        z = np.dot(X, theta)\n",
        "        h = sigmoid(z)\n",
        "        J = (-1 / m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y.T), np.log(1 - h)) )\n",
        "        theta = theta - (alpha / m) * (np.dot(X.T, (h - y)))\n",
        "    J = float(J)\n",
        "    return J, theta"
      ],
      "metadata": {
        "id": "ciL1VkDj4uZq"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "J, theta = gradientDescent(X_train, y_train, np.zeros((3, 1)), 1e-9, 1500)\n",
        "print('cost: ', J)\n",
        "print('theta: ', theta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmhrH1L46czA",
        "outputId": "f4852dc3-1323-46eb-fd3a-7c2d7ee4f27d"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost:  0.6847755515628218\n",
            "theta:  [[ 1.09696961e-08]\n",
            " [ 5.26825749e-05]\n",
            " [-9.61885504e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def step_function(y_pred):\n",
        "    if y_pred < 0.5:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'"
      ],
      "metadata": {
        "id": "GaPG4hfGE7Kc"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_tweet(tweet, theta):\n",
        "    '''\n",
        "    '''\n",
        "    # extract the features of the tweet and store it into x\n",
        "    x = embed_tweet(tweet)\n",
        "    \n",
        "    # make the prediction using x and theta\n",
        "    y_pred = sigmoid(np.dot(x, theta))\n",
        "    \n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "91USuqA29yqZ"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following code blocks were taken from the NLP Course by Deeplearning.ai I wanted to use them to check my work."
      ],
      "metadata": {
        "id": "V-qMTyt_PnEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
        "    print( (tweet, predict_tweet(tweet, theta))) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2PHzlufETTp",
        "outputId": "3995c482-dd02-40c0-e6a2-90612bc02c71"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I am happy', array([[0.49535578]]))\n",
            "('I am bad', array([[0.50120451]]))\n",
            "('this movie should have been great.', array([[0.40863361]]))\n",
            "('great', array([[0.49210544]]))\n",
            "('great great', array([[0.48421482]]))\n",
            "('great great great', array([[0.47633206]]))\n",
            "('great great great great', array([[0.46846107]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logistic_regression(test_x, test_y, theta, predict_tweet=predict_tweet):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        test_x: a list of tweets\n",
        "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
        "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
        "        theta: weight vector of dimension (3, 1)\n",
        "    Output: \n",
        "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # the list for storing predictions\n",
        "    y_hat = []\n",
        "    \n",
        "    for tweet in test_x:\n",
        "        # get the label prediction for the tweet\n",
        "        y_pred = predict_tweet(tweet, theta)\n",
        "        \n",
        "        if y_pred > 0.5:\n",
        "            # append 1.0 to the list\n",
        "            y_hat.append(1.0)\n",
        "        else:\n",
        "            # append 0 to the list\n",
        "            y_hat.append(0.0)\n",
        "\n",
        "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
        "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
        "    accuracy = np.sum(np.array(y_hat) == test_y.flatten(order='C')) / len(y_hat)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "yPlGcli2EXHg"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_accuracy = test_logistic_regression(X_test, y_test, theta)\n",
        "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-MVuuzrMOB8",
        "outputId": "3db0d0a0-d1c5-4dd3-a838-c8777d4ac18d"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic regression model's accuracy = 0.5028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sHGsF7gTMUrq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}